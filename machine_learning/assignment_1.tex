\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,graphicx,mathtools,tikz,hyperref}

\usetikzlibrary{positioning}

\newcommand{\n}{\mathbb{N}}
\newcommand{\z}{\mathbb{Z}}
\newcommand{\q}{\mathbb{Q}}
\newcommand{\cx}{\mathbb{C}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\ita}[1]{\textit{#1}}
\newcommand{\com}[2]{#1\backslash#2}
\newcommand{\oneton}{\{1,2,3,...,n\}}
\newcommand{\abs}[1]{|#1|}
\newcommand{\card}[1]{|#1|}
\newcommand\idea[1]{\begin{gather*}#1\end{gather*}}
\newcommand\ef{\ita{f} }
\newcommand\eff{\ita{f}}
\newcommand\proofs[1]{\begin{proof}#1\end{proof}}
\newcommand\inv[1]{#1^{-1}}
\newcommand\set[1]{\{#1\}}
\newcommand\en{\ita{n }}
\newcommand\nullity{\text{nullity}}
\newcommand{\vbrack}[1]{\langle #1\realangle}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\res}[2]{#1\bigg\rvert_{#2}}
\newcommand{\im}[0]{\text{im}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\spn}[0]{\text{span}}
\newcommand{\colsp}{\text{Colsp}}
\newcommand{\nullsp}{\text{Nullsp}}
\newcommand{\rk}{\text{rank}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{defn}{Definition}[thm]
\newtheorem*{remark}{Remark}
\newtheorem{prop}{Proposition}[thm]
\newtheorem{example}{Example}[thm]
\newtheorem{question}{Question}
\newtheorem{lemma}{Lemma}[thm]
\newtheorem{exercise}{Exercise}[thm]

\hypersetup{
  colorlinks,
  linkcolor=blue
}
\setcounter{tocdepth}{3}% Show ToC content up to level 3 (\subsubsection)

           
\begin{document}
\begin{question}[3 points]
  In class we described the Least Squares algorithm as a learning algorithm that minimizes empirical risk.
  See PDF summary, and adopt the same notation. In at most 3 sentences answer the following question, backing up each claim you make by either naming a theorem from Linear Algebra or referring to facts from the PDF summary: Suppose $m < d < \infty$ then what can you conclude about the rank of the $d \times d$ matrix $A$ and why does this imply that there are infinitely many $w$ that minimize empirical risk? 
\end{question}
\begin{proof}

  We conclude that $\rk(A) < d$.
  \footnote{Let $B, C$ be $m \times n$ and $n \times k$ dimensional matrices, respectively. Let $w_1, \dots, w_{d_1}$ be a basis for the column space of $B$ and $v_1, \dots, v_{d_2}$ be a basis for the column space of $C$. Then for any $v \in \real^k$ there exist $\alpha_i, \beta_j \in \real$ such that
  \begin{align*}
   \sum\limits_{i=1}^{d_1}\beta_iw_i = BCv = B(Cv) =  B(\sum\limits_{i=1}^{d_2}\alpha_iv_i) = \sum\limits_{i=1}^{d_2}\alpha_iBv_i 
  \end{align*}
  Hence, since a basis is a minimal spanning set, and both $\set{w_i}_{i=1}^{d_1}$ and $\set{Bv_i}_{i=1}^{d_2}$  span the column space of $BC$, we get $\rk(BC) \leq \min(\set{\rk(B), \rk(C)})$. Since $\rk(\X^T) = \rk(\X)$ (I will take for granted that the column rank is the same as the row rank), we conclude $\rk(A) = \rk(\X^T\X) \leq \rk(\X) \leq m < d$.
  }
  Let $v \in \nullsp(A)$ be non-zero
  \footnote{
  By the rank-nullity theorem, we get that $\nullity(A)>0$. Hence there exists some non trivial element $ v \in \nullsp(A)$.
  }
  and $w$ a solution of $Aw=b$,
  \footnote{
  The empirical risk of $f_w$ is measured by $R(f_w) = \norm{\Y - \X w}^2$. Taking derivatives one finds that this is minimized when $Aw=b$, for $A=\X^T X$ and $b=\X^T \Y$. \\
  \begin{indent}
    We now show that a solution exists. Let $v \in \nullsp(A)$, then for any $u \in \real^d$ we get $v \cdot Au = v^T A u = Av \cdot u = 0$, so $\nullsp(A) \subseteq \colsp(A)^{\perp}$. Conversely for $v \in \colsp(A)^{\perp}$, let $e_i =(0, \dots, 1, \dots, 0)^T \in \real^d$ be the $i$th standard basis vector, then $0 = v \cdot Ae_i = Av \cdot e_i$. Thus, $Av = 0$, and $v \in \nullsp(A)$. Since $b=\X^T\Y$, for $v \in \nullsp(A)$ we have $b \cdot v = (\X^T \Y) \cdot v = \Y^T (\X v) = 0$. Hence $b \in \nullsp(A)^{\perp} = \colsp(A)^{\perp^\perp} = \colsp(A)$, and there exists some non-zero $w \in \real^d$ such that $Aw = b$.
  \end{indent}
  }
  then we get infinitely many solutions $w_{\alpha} = w + \alpha v$, for $\alpha \in \real$.
  \footnote{
  $Aw_{\alpha} = A(w + \alpha v) = Aw + \alpha Av = b$. More generally if $v_1, \dots, v_k$ is a basis for the nullspace of $A$, then for any $\alpha_1, \dots, \alpha_k$ we have
  $A(w+\sum\limits_{i=1}^k \alpha_iv_i) = Aw + \sum\limits_{i=1}^k \alpha_i Av_i = Aw$.
  }
\end{proof}
\newpage
\begin{question}[4 points]
  In order to analyze runtime one needs first to assume a model of computation.  You should assume for this question that every addition or multiplication of two real numbers takes one unit of time and all other computational (and book-keeping) activities are essentially instantaneous (time  zero). By  counting  additions and multiplications, you  will therefore obtain an amount of time that the algorithm takes under the assumed model of computation. In particular, you should express this time asymptotically as $O(f)$ for some function $f$ of $n$ and $d$ given training data as in the previous question and supposing a new data point $(x_{n+1}, y_{n+1})$ arrives.  Assume that inversion of a $d \times d$ matrix takes time $O(d^3)$
  \begin{enumerate}
  \item (3.5 points) With all the above assumptions, show that Recursive Least Squares will take time $O(d^3)$ to find the optimal $w_{*} = w_{n+1}$ knowing the previous $w_{*}=w_n$ and matrices from that previous computation (Hint:  see that the time taken for updating the gain matrix dominates this computation). On the other hand, show that performing Ordinary Least Squares on the full data set $(x_i, y_i)$, $ i= 1, . . . n+ 1$ would take time $O((n+ 1)(d+d^2) +d^3) =O(nd^2+d^3)$.
  \item
    (0.5 point) What situations does this suggest should be handled by Recursive Least Squares instead of Ordinary Least Squares?  Justify your answer.
  \end{enumerate}
\end{question}
\begin{proof}
  (PART 1) We'll find the time complexity of each subexpression, first assuming $w_n$ is cached, then not, and add them together:
  \begin{align*}
    w_{n+1} = w_{n} +Ke_1,
  \end{align*}
  \begin{itemize}
  \item
    Computing $w_n$: Zero run time.
  \item
    Computing $K$: We are given, in handout ols-rls-2021, the following recipe for computing $K$: 
  \begin{enumerate}
  \item
    compute $\inv{P}_{n+1} = \inv{P}_n + C_1^TC_1$ ,
  \item
    invert this $d \times d$ matrix to get $P_{n+1}$,
  \item
    compute $P_{n+1}C_1^T$.
  \end{enumerate}
  We find the time complexity of each step:
  \begin{enumerate}
  \item
    Computing  $P_n$ costs us nothing. Inverting it, since it's a $d \times d$ matrix, costs us $d^3$. By assumption $C_1 = x_{n+1}$, so $C_1^T C_1 = x_{n+1} \cdot x_{n+1}$ takes at most $d+ d-1 = 2d-1$ to evaluate ($d$ for multiplying the components, $d-1$ for summing them). Finally, it takes $1$ unit of time to add $P_n$ to $C_1^T C_1$. Thus, computing $P_{n+1}^{-1}$ takes no more than $d^3$ + $2d-1$ + $1= d^3 + 2d \leq 3d^3$, so $\inv{P}_{n+1}$ is $O(d^3)$.
  \item
    Inverting $P_{n+1}^{-1}$ costs us $d^3$, so this step is $O(d^3)$.
  \item
    Writing $P_{n+1}C_1^T = (r_i \cdot x_{n+1})^T$, for $r_i$ the $i$th row of $P_{n+1}$, we see that it takes at most $d(2d-1) = 2d^2-d \leq d^3$, so that's $O(d^3)$. 
  \end{enumerate}
  Thus, computing $K$ is $O(d^3) + O(d^3) + O(d^3) = O(3d^3) = O(d^3)$.
\item Computing $e_1$:
  We have $e_1 = b_1 - C_1 w_n$. Since $C_1 = x_{n+1}$ is just a vector of length $d$, we have $C_1 w_n = x_{n+1} \cdot w_n$ costs $2d-1$. Retrieving the value of $b_1$ from memory costs nothing, and taking the difference costs $1$. Thus this step costs
  $2d$, which is $O(d^3)$.
\item
  Add
  $w_n$ and $Ke_1$: This is $O(1)=O(d^3)$.
  \end{itemize}
  The time complexity of calculating $w_{n+1}$ when you're caching values is therefore $O(d^3) + O(d^3) + O(d^3) = O(3d^3) = O(d^3)$.
\bigbreak
  To find the time it would take to compute $w_{n+1}$ using ordinary least squares, we compute the following:
\begin{align*}
  w_{n+1} = (\X^T\X)^{-1} \X^T \Y
\end{align*}
\begin{itemize}
\item
  Computing $(\X^T\X)^{-1}$: $\X$ is an $n+1 \times d$ matrix. Write $\X=(c_i)$, where $c_i$ is the $i$th column of $X$. Then $\X^T\X=(c_i\cdot c_j)$, which
  is $d^2$ dot-products of vectors of length $(n+1)$. Each dot-product takes $2n+1$ time ($n+1$ to multiply all the entries component-wise and $n$ to sum over them). This takes $d^2(2n+1)$. Inverting the product takes $d^3$, so all together this step is $d^3 + d^2(2n+1) \leq d^3 + 2d^2(n+1) \leq 2(d^3 + d^2(n+1))$, so this step is
  $O(d^3 + d^2(n+1))$.
\item
  Computing $\X^T\Y$: Write $\X^T = (r_i)$, where $r_i$ is the $i$th row of $\X^T$. Then, as $\X^T$ is a $d \times n+1$ matrix and $\Y$ has length $n+1$, computing $\X^T\Y = (r_i \cdot \Y)$ takes $d(2n+1) \leq 2d(n+1)$, so this step is $O(d(n+1))$.
\item
  Computing the product: $(\X^T\X)^{-1}$ is a $d \times d$ matrix and $\X^T\Y$ has length $d$. Thus it takes $d(2d-1) \leq 2d^2$. This step is $O(d^2)$.
\end{itemize}
Putting it all together, it takes
\begin{align*}
  O(d^3 + d^2(n+1)) + O(d(n+1)) + O(d^2) &= O(d^3 + d^2(n+1) + d(n+1) + d^2) \\
  &= O((n+1)(d^2 + d) + d^3 + d^2) \\
  &= O((n+1)(d^2 + d) + d^3) \\
\end{align*}
This last equality holds because
\begin{align}
  (n+1)(d^2+d) + d^3 + d^2 \leq C((n+1)(d^2+d)+d^3)
\end{align}
trivially holds when $d=0$, for any constant $C>0$; and holds for $d > 0$ if and only if
\begin{align*}
  C \geq 1 + \cfrac{d^2}{d^3(n+1)(d^2+d)}.
\end{align*}
Taking limits, we see the right hand side converges to $1$. Let $C_{\epsilon} := 1+\epsilon$. Then there exists some $N$ such that (1) holds for all $n > N$.
\bigbreak
  Next we check that
\begin{align*}
   O((n+1)(d^2 + d) + d^3) = O(nd^2 + d^3).
\end{align*}
We have
\begin{align}
  (n+1)(d^2 + d) + d^3 \leq C(nd^2 + d^3),
\end{align}
which trivially holds for $d=0$; and for $d > 0$ it holds if and only if 
\begin{align*}
  C &\geq \cfrac{(n+1)(d^2+d)}{nd^2+d^3} + \cfrac{d}{n+d} \\
  &= \cfrac{(1+\frac{1}{n})(d+1)}{d+\frac{d^2}{n}} + \cfrac{d}{n+d}.
\end{align*}
Taking limits we find that the right hand side converges to $\cfrac{d+1}{d}$. Fix some $\epsilon>0$, and let $C_{\epsilon} = \cfrac{d+1}{d} + \epsilon$. Then there exists some $N$ such that (2) holds for all $n>N$. \bigbreak

(PART 2) Any time that the program has to interact with the outside world over long periods of time recursive least squares should be used (or some algorithm that memoizes/caches). We didn't look at how long it'd take to run recursive least squares on a data set without memoization. It'd probably be way longer than ordinary least squares. Hence, if the program doesn't have to interact with the outside world a lot, i.e. if you're going to only run the computation a few times, you'd better use ordinary least squares. If your program has to interact with the outside world a lot, like if new data is always coming in and you're re-computing the same things over and over to make use of that new data, then use recursive least squares.

\end{proof}
\begin{question}
  Read Chapter 3 of Tom Mitchell's book Machine Learning to introduce yourself to the Naive Bayes algorithm for producing a classifier. In particular pages 8-10 derive the form of $P(Y|X)$ used by Naive Bayes under simple Gaussian assumptions - so-called Gaussian Naive Bayes (GNB) - and show it is exactly the same $P(Y|X)$ used by logistic regression. Now consider instead slightly more general assumptions where the Gaussians have standard deviation that may depend on the class (value of $Y$). Derive the form of $P(Y|X)$ under this new more general version of GNB and say whether it still corresponds to logistic regresssion. Finally consider a less naive algorithm where conditional independence of the random variables in $X = (X_1, ..., X_n)$ is not assumed - specifically suppose $P(X|Y=k)$ is multivariate normal with mean depending on k, but covariance matrix now NOT depending on k. Derive $P(Y|X)$ and say if it has the same form as in logistic regression.
\end{question}
\begin{proof}
  Our assumptions are:
  \begin{itemize}
  \item
    $Y$ is boolean, governed by a Bernoulli distribution, with parameter $\pi = P(Y = 1)$.
  \item
    $X = (X_1, \dots, X_n)$, where each $X_i$ is a continuous random variable.
  \item
    For each $X_i, P(X_i | Y=y_k)$ is a Gaussian distribution of the form $N(\mu_{ik}, \sigma_{ik})$.
  \item
    For all $i$ and $j \neq i$, $X_i$ and $X_j$ are conditionally independent from $Y$.
  \end{itemize}
  Assuming these, let's begin our derivation:
  \begin{align}
    P(Y=1|X)
    &= \cfrac{P(Y=1)P(X|Y=1)}{P(Y=1)P(X|Y=1) + P(Y=0)P(X|Y=0)} \\
    &= \cfrac{1}{1 + \cfrac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)}} \\
    &= \cfrac{1}{1+\exp(\ln\cfrac{P(Y=0)P(X|Y=0)}{P(Y=1)P(X|Y=1)})} \\
    &= \cfrac{1}{1+\exp(\ln\cfrac{P(Y=0)\prod\limits_{i=1}^nP(X_i|Y=0)}{P(Y=1)\prod\limits_{i=1}^nP(X_i|Y=1)})} \\
    &= \cfrac{1}{1+\exp(\ln\cfrac{P(Y=0)}{P(Y=1)} + \sum\limits_{i=1}^n\ln\cfrac{P(X_i|Y=0)}{P(X_i|Y=1)})} \\
    &= \cfrac{1}{1+\exp(\ln\cfrac{1-\pi}{\pi} + \sum\limits_{i=1}^n\ln\cfrac{P(X_i|Y=0)}{P(X_i|Y=1)})}.
  \end{align}
  We have
  \begin{align*}
    \sum\limits_{i=1}^n\ln\cfrac{P(X_i|Y=0)}{P(X_i|Y=1)}
    &= \sum\limits_{i=1}^n\ln
    \cfrac{\frac{1}{\sqrt{2\pi\sigma_{i0}^2}}\exp(\frac{-(X_i-\mu_{i0})^2}{2\sigma_{i0}^2})}{\frac{1}{\sqrt{2\pi\sigma_{i1}^2}}\exp(\frac{-(X_i-\mu_{i1})^2}{2\sigma_{i1}^2})} \\
    &= \sum\limits_i \ln(\frac{\sigma_{i1}}{\sigma_{i0}}\exp(\frac{-(X_i-\mu_{i0})^2}{2\sigma_{i0}^2} + \frac{(X_i-\mu_{i1})^2}{2\sigma_{i1}^2})) \\
    &=\sum\limits_i \ln(\frac{\sigma_{i1}}{\sigma_{i0}}) +
    \frac{\sigma_{i0}^2-\sigma_{i1}^2}{2\sigma_{i0}^2\sigma_{i1}^2}X_i^2 + \cfrac{\mu_{i0}\sigma_{i1}^2-\mu_{i1}\sigma_{i0}^2}{\sigma_{i0}^2\sigma_{i1}^2}X_i +
    \cfrac{\sigma_{i0}^2\mu_{i1}^2 - \sigma_{i1}^2\mu_{i0}^2}{2\sigma_{i0}^2\sigma_{i1}^2}) \\
    &=\sum\limits_i \ln(\frac{\sigma_{i1}}{\sigma_{i0}}) +
    (\cfrac{1}{2\sigma_{i1}^2}-\cfrac{1}{2\sigma_{i0}^2})X_i^2+ (\cfrac{\mu_{i0}}{\sigma_{i0}^2}-\cfrac{\mu_{i1}}{\sigma_{i1}^2})X_i +
    (\cfrac{\mu_{i1}^2}{2\sigma_{i1}^2} - \cfrac{\mu_{i0}^2}{2\sigma_{i0}^}) \\
    &=\sum\limits_i (\cfrac{1}{2\sigma_{i1}^2}-\cfrac{1}{2\sigma_{i0}^2})X_i^2+ (\cfrac{\mu_{i0}}{\sigma_{i0}^2}-\cfrac{\mu_{i1}}{\sigma_{i1}^2})X_i +
    (\cfrac{\mu_{i1}^2}{2\sigma_{i1}^2} - \cfrac{\mu_{i0}^2}{2\sigma_{i0}^} + \ln(\frac{\sigma_{i1}}{\sigma_{i0}}))\\
    &= \sum\limits_i \alpha_{i2} X_i^2 + \alpha_{i1} X_i + \widetilde{\alpha_{i0}},
  \end{align*}
  for
  \begin{align*}
    \widetilde{\alpha_{i0}} &= \cfrac{\mu_{i1}^2}{2\sigma_{i1}^2} - \cfrac{\mu_{i0}^2}{2\sigma_{i0}^} + \ln(\frac{\sigma_{i1}}{\sigma_{i0}}), \\
    \alpha_{i1} &= \cfrac{\mu_{i0}}{\sigma_{i0}^2}-\cfrac{\mu_{i1}}{\sigma_{i1}^2}, \\
    \alpha_{i2} &= \cfrac{1}{2\sigma_{i1}^2}-\cfrac{1}{2\sigma_{i0}^2}.
  \end{align*}
  Let 
  \begin{align*}
    \alpha_{00} = \ln\cfrac{1-\pi}{\pi} + \sum\limits_i^n\widetilde{\alpha_{i0}}.
  \end{align*}
  Then, substituting in our polynomial expression for $X_i$, and continuing from equation (8):
  \begin{align*}
    P(Y=1|X) = \cfrac{1}{1+\exp(\alpha_{00} + \sum\limits_{i=1}^n \alpha_{i2}X_i^2 + \alpha_{i1}X_i)},
  \end{align*}
  and from this we get:
  \begin{align*}
    P(Y=0|X) &= 1 - P(Y=1|X) \\
    &= \cfrac{\exp(\alpha_{00} + \sum\limits_{i=1}^n \alpha_{i2}X_i^2 + \alpha_{i1}X_i)}{1+\exp(\alpha_{00} + \sum\limits_{i=1}^n \alpha_{i2}X_i^2 + \alpha_{i1}X_i)}.
  \end{align*}
  This more general GNB does not correspond to logistic regression, since if $\sigma_{i0} \neq \sigma_{i1}$, for any $0 \leq i \leq n$, then $\alpha_{i2} \neq 0$ and there is a degree two term in the denominator of the parametrization of $P(Y=1|X)$. If $\forall i, \sigma_{i0} = \sigma_{i1}$, then $\forall i, \alpha_{2i}=0$, and we are back to the case where the conditional variance is independent of $Y$, i.e. back to the case where $GNB$ corresponds to logistic regression.
  \bigbreak
  Next, we derive $P(Y|X)$ under the following assumptions:
  \begin{itemize}
  \item
    $Y$ is boolean, governed by a Bernoulli distribution, with parameter $\pi = P(Y = 1)$.
  \item
    $X = (X_1, \dots, X_n)$, where each $X_i$ is a continuous random variable.
  \item
    $P(X | Y=k) \sim \mathcal{N}(\mu_{k}, \Sigma)$ is multivariate normal with mean depending on $k$, and variance independent from $k$, where
    \begin{align*}
      \mu_k
      &= E[X] = (E[X_1], \dots, E[X_n])^T = (\mu_{1k}, \dots, \mu_{nk})^T \\
      \Sigma
      &= (\text{Cov}[X_i, X_j])_{ij}
    \end{align*}
  \end{itemize}  
\end{proof}
\end{document}

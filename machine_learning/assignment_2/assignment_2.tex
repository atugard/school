\documentclass[12pt]{article}
 
\usepackage[margin=1in]{geometry} 
\usepackage{amsmath,amsthm,amssymb,graphicx,mathtools,tikz,hyperref}

\usetikzlibrary{positioning}

\newcommand{\n}{\mathbb{N}}
\newcommand{\z}{\mathbb{Z}}
\newcommand{\q}{\mathbb{Q}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\cx}{\mathbb{C}}
\newcommand{\real}{\mathbb{R}}
\newcommand{\field}{\mathbb{F}}
\newcommand{\ita}[1]{\textit{#1}}
\newcommand{\com}[2]{#1\backslash#2}
\newcommand{\oneton}{\{1,2,3,...,n\}}
\newcommand{\abs}[1]{|#1|}
\newcommand{\card}[1]{|#1|}
\newcommand\idea[1]{\begin{gather*}#1\end{gather*}}
\newcommand\ef{\ita{f} }
\newcommand\eff{\ita{f}}
\newcommand\proofs[1]{\begin{proof}#1\end{proof}}
\newcommand\inv[1]{#1^{-1}}
\newcommand\set[1]{\{#1\}}
\newcommand\en{\ita{n }}
\newcommand\nullity{\text{nullity}}
\newcommand{\vbrack}[1]{\langle #1\realangle}
\newcommand{\norm}[1]{\lVert #1 \rVert}
\newcommand{\res}[2]{#1\bigg\rvert_{#2}}
\newcommand{\im}[0]{\text{im}}
\newcommand{\X}{\mathcal{X}}
\newcommand{\Y}{\mathcal{Y}}
\newcommand{\spn}[0]{\text{span}}
\newcommand{\colsp}{\text{Colsp}}
\newcommand{\nullsp}{\text{Nullsp}}
\newcommand{\rk}{\text{rank}}
\newtheorem{thm}{Theorem}[section]
\newtheorem{defn}{Definition}[thm]
\newtheorem*{remark}{Remark}
\newtheorem{prop}{Proposition}[thm]
\newtheorem{example}{Example}[thm]
\newtheorem{question}{Question}
\newtheorem{lemma}{Lemma}[thm]
\newtheorem{exercise}{Exercise}[thm]

\hypersetup{
  colorlinks,
  linkcolor=blue
}
\setcounter{tocdepth}{3}% Show ToC content up to level 3 (\subsubsection)

           
\begin{document}

\title{MAT5314: Assignment 2}
\author{David Draguta
  \\University of Ottawa\\ Prof: Dr. Maia Fraser } 
 
\maketitle

\begin{question}[3 points]
  One common preprocessing in machine learning is to centre the data. In this problem we show this can be related to working with an (unpenalized) off-set term in the solution. Suppose we are in the Euclidean regression setting and examples come from $\X \times \Y$, where $\X=\real^d$ is input and $\Y=\real$ output. Consider the usual (Tikhonov) regularized least squares with standard inner product in $\real^d$, but now with an additional unpenalized offset term $b$,
  \begin{align*}
    \min\limits_{w \in \real^d, b \in \real}
    \left\{
    \cfrac{1}{n} \sum\limits_{i=1}^n(\langle w, x_i \rangle + b - y_i)^2 + \lambda\norm{w}^2
    \right\}
    ,
  \end{align*}
  and let $(w^*, b^*)$ be the solution of this problem. Write $E(w,b)$ for the objective function. For $i \in [n]$, denote by $x_i^c = x_i - \bar{x}, y_i^c = y_i - \bar{y}$ the centred data, where $\bar{x}, \bar{y}$ are the output and input means respectively. Consider now the following optimization
  \begin{align*}
    \min\limits_{w \in \real^d}
    \left\{
    \cfrac{1}{n} \sum\limits_{i=1}^n(\langle w, x_i^c \rangle - y_i^c)^2 + \lambda\norm{w}^2
    \right\}
    ,
  \end{align*}
  and write $E^c(w)$ for this new objective function.
  \begin{itemize}
  \item Show that $E(w^*, b^*) \leq E^c(w)$ for all $w \in \real^d$ (Hint: try relating the two objective functions).
  \item Show the second optimization can be re-expressed as a constrained version of the first, where $b$ is constrained to satisfy a specific relationship with $w$.
  \item Show that $w^*$ in fact solves the second optimization (Hint: for fixed $w$ consider $E(w,b)$ as a function of $b$ and check where its minimum occurs).
  \end{itemize}
  \begin{proof}
    We have
    \begin{align*}
      \cfrac{1}{n}\sum\limits_{i=1}^nx_i^c = \cfrac{1}{n}\sum\limits_{i=1}^n(x_i - \bar{x}) = 0,
    \end{align*}
    and
    \begin{align*}
      \cfrac{1}{n}\sum\limits_{i=1}^ny_i^c = \cfrac{1}{n}\sum\limits_{i=1}^n(y_i - \bar{y}) = 0,
    \end{align*}
    thus
    \begin{align*}
      E(w,b) &= \cfrac{1}{n}\sum\limits_{i=1}^n(\langle w, x_i \rangle + b - y_i)^2 + \lambda \norm{w}^2 \\
      &= \cfrac{1}{n}\sum\limits_{i=1}^n(\langle w, x_i - \bar{x} + \bar{x} \rangle + b - (y_i - \bar{y}) -\bar{y})^2 +  \lambda \norm{w}^2 \\
      &= \cfrac{1}{n}\sum\limits_{i=1}^n(\langle w, x_i^c + \bar{x} \rangle + b - y_i^c -\bar{y})^2 +  \lambda \norm{w}^2 \\
      &= \cfrac{1}{n}\sum\limits_{i=1}^n(\langle w, x_i^c \rangle -y_i^c + \langle w, \bar{x} \rangle + b -\bar{y})^2 +  \lambda \norm{w}^2 \\
      &= E^c(w)
      + 2 \cfrac{1}{n}\sum\limits_{i=1}^n(\langle w, x_i^c \rangle -y_i^c)(\langle w, \bar{x} \rangle + b -\bar{y}) \
      + \cfrac{1}{n}\sum\limits_{i=1}^n (\langle w, \bar{x} \rangle + b -\bar{y})^2\\
      &= E^c(w)
      + 2 (\langle w, \cfrac{1}{n}\sum\limits_{i=1}^n x_i^c \rangle - \cfrac{1}{n}\sum\limits_{i=1}^n y_i^c)(\langle w, \bar{x} \rangle + b -\bar{y}) \
      + (\langle w, \bar{x} \rangle + b -\bar{y})^2 \\
      &= E^c(w) + (\langle w, \bar{x} \rangle + b -\bar{y})^2.
    \end{align*}
    At critical values of $E$ we have $\nabla_{w,b}E(w,b) = 0$. In particular
    \begin{align*}
      0 = \nabla_bE(w,b)
      &= \nabla_b E^c(w) + \nabla_b (\langle w, \bar{x} \rangle + b -\bar{y})^2 \\
      &= 2(\langle w, \bar{x} \rangle + b -\bar{y}),
    \end{align*}
    so at $(w, b^*)$ we have
    \begin{align*}
      \langle w, \bar{x} \rangle + b^* -\bar{y} = 0.
    \end{align*}
    Thus, we get
    \begin{align*}
      E(w^*,b^*)  \leq E(w,b^*) = E^c(w).
    \end{align*}
    We can express the second optimization problem as
    \begin{align*}
      \min\limits_{w \in \real^d, b \in \real} E(w,b) \text{ such that } \langle w, \bar{x} \rangle + b = \bar{y}.
    \end{align*}
    Thus, since $(w^*, b^*)$ minimizes $E(w,b)$, and since $\langle w, \bar{x} \rangle + b = \bar{y}$ holds at $(w^*, b^*)$, we see that $w^*$ indeed solves the second optimization problem.
  \end{proof}
  \begin{question}
    Given training data $(x_1, y_1) = (-1, -1), (x_2, y_2) = (-0.8, 1), (x_3, y_3) = (1,1)$ is there a separating hyperplane? Suppose we nevertheless run the variant of SVM with slack variables that we posed for the non-separable case. Are there some choices of $C$ which would result in the algorithm picking a non-separating hyperplane at x = 0? Justify your answer.
    
  \end{question}
  \begin{proof}
    Our hyperplane can be a point anywhere in $(-1, -0.8)$. The point maximizing the margin is at $-0.9$. Then, everything to the left of that point has label $-1$ and to the right has label $+1$. We could also see this by using support vectors and the KKT conditions. We have support vectors $x_1, x_2$. The KKT conditions give us the following constraints on $w,b$ and $\alpha$:
    \begin{align*}
      w &= \alpha_1 - 0.8\alpha_2, \\
      \alpha_1 &= \alpha_2, \\
      -1(-w + b) &= 1 \\
      1(-0.8w + b) &= 1 .
    \end{align*}
    One finds that $(w,b)=(10,9)$ is the unique solution, which corresponds to the hyperplane $H_{w,b} = \set{x: wx+b = 0} = \set{-0.9}$.
    \bigbreak
    The hyperplane at $x=0$ is described by $(w,b) = (1,0)$. The SVM algorithm (you can check directly with the svm.py file submitted with this assignment. Just uncomment solnq2 and make sure C is set to 5) with slack variables returns $\alpha_3=0$ irrespective of our choice of $C$; for $0<C<50$ it returns $ \alpha_1, \alpha_2 = C$, and for  $C > 50$, it returns $ \alpha_1= \alpha_2 = 50$. We assume $C<50$, so
    \begin{align*}
      w = \alpha_1 y_1 x_1 + \alpha_2 y_2 x_2 = C( y_1x_1 + y_2x_2) = C( 1 - 0.80) = C/5 = 1,
    \end{align*}
    which holds if and only if $C=5.$
 
  \end{proof}
\end{question}
\begin{question}
  \begin{itemize}
  \item 
    \textbf{Option B}. Hand-code an SVM algorithm for the non-separable case using a general-purpose quadratic programming solver. You should use an existing package/function for the solver but not for the svm itself. Specifically, you should write a function that takes as input the data and C (the constant in $C \sum\limits_i \xi_i$ of the objective function), that calls the quadratic solver to perform the optimization we stated for the non-separable case, and then outputs the estimated $(w_*, b_*)$ as well as the size of the margin and the support vectors. Include your code, which must not only work, but be well-structured and well-commented for full marks. Now, try using your algorithm to check, for each pair of non-class attributes, is this pair sufficient to linearly separate Iris Setosa from the other species, e.g. taking just the attributes sepal width and sepal length to be $x \in X$, is Iris Setosa linearly separable? Continue like this for all 6 possible pairs. Each time you run your algorithm, also run an existing svm function to compare the results. Report and comment on your findings, in at most half a page.
  \item
    \textbf{Option C}. Consider a dataset of 6 points in the plane with labels $\pm$ as follows: the points $(1,1), (2,2), (0,2)$ are label $+1$, while $(0,1),(1,0),(-1,0)$ are labeled $-1$. Are these points linearly separable? By inspection (no need to run an SVM) give equations for the maximum margin hyperplanes. Give the support vectors. For each of the support vectors, consider removing just that point from the dataset and say if and how this will change the margin. In general, for an arbitrary linearly separable dataset, if we remove one data point from the set is it possible for the margin to increase? Prove your answers, either by giving an example of a dataset where this happens, or proving it cannot happen.
  \end{itemize}
\end{question}
\begin{proof}  \bigbreak
  \textbf{ Option B:}
  Let $x_0 = \text {sepal length}, x_1 = \text{sepal width}, x_2 = \text{petal length},$ and $x_3 = \text{petal width}$. To check if the pairs are linearly separable, we put a high value of C
  (in the computations below $C = 100000000$ was used, as to drive the slack variables to zero in the minimization of the objective function.)
  \bigbreak
  For inputs $x_0,x_1,x_2,x_3$, we have $w_* = (0.05, -0.52, 1, 0.46), b_* = -1.45, \text{margin} = 0.82, \\
  \text{support vectors} = (5.1,3.3,1.7,0.5), (4.5,2.3,1.3,0.3), (5.1,2.5,3,1.1)$.
  \bigbreak
  For $x_0, x_1$, we get $w_* = (8.57, -7.14), b_* = -23.1, \text{margin} = 0.09, \\
  \text{support vectors} = (5.5,3.5), (4.5, 2.3), (4.9, 2.5)$.
  \bigbreak
  For $x_0, x_2$, we have $w_* = (0.01, 1.82), b_*=-4.53, \text{margin} = 0.55, \\
  \text{support vectors} = (5.1,1.9), (5.1, 3)$.
  \bigbreak
  For $x_0, x_3$, we have $w_* = (0, 5), b_* = -4, \text{margin} = 0.2, \\
  \text{support vectors} = (5, 0.6), (4.9,1), (5, 1), (6,1), (5.8, 1), (5.7, 1), (5.5, 1), (5, 1).$
  \bigbreak
  For $x_1, x_2$, we have $w_* = (-0.69, 1.26), b_* = -1.06, \text{margin} = 0.70, \\
  \text{support vectors} = (3.4,1.9), (2.3,1.3), (2.5,3)$
  \bigbreak
  For $x_1, x_3$, we have $w_* = (-0.83, 3.33), b_* = -0.08, \text{margin} = 0.29, \\
  \text{support vectors} = (2.3,0.3), (3.5, 0.6), (2.7, 1)$
  \bigbreak
  For $x_2, x_3$, we have $w_* = (1.29, 0.82), b_* = -3.79, \text{margin} = 0.65, \\
  \text{support vectors} = (1.9,0.4), (3, 1.1)$
  \bigbreak
  We find that Iris Setosa is linearly seperable from the other two in all cases except possibly when we train on sepal length and sepal width only. In that case the margin is 0.09, which
  might be due to inaccuracies in the results produced by the quadratic program solver, so that the data might not be separable. Otherwise, the data is separable,with the highest margin when we take the full data set, and the next two highest margins
  when we train on sepal width and petal length, and petal length and petal width. We have an abnormally high amount of support vectors when we train on sepal length and petal width, and a low margin. It seems like
  sepal length and petal width are similar across all three kinds of flower, and that perhaps most of these support vectors lie within the region enclosed by the marginal hyperplanes (and so have non-zero slack variables). As $C$ is high, we can be confident that nevertheless they must deviate only by a little from the marginal hyperplanes (if they do at all).
  \bigbreak
  \textbf{ Option C:} 
  The data is separable. Let's name the data: $(x_1,y_1) = ((1,1),1), (x_2, y_2) = ((0,2),1), (x_3,y_3) = ((0,1), -1), (x_4,y_4) =  ((1,0), -1), (x_5,y_5) = ((2,2),1), $ and $ (x_6,y_6) = ((-1,0), -1)$.
  We have support vectors $x_1, x_2, x_3, x_4$. The separating hyperplane is a line with equation $y=-x+3/2$ or $(x,y) \cdot (1,1) - 3/2 = 0 $. Now, we want $x_3$ to lie on the hyperplane
  satisfying $w \cdot x + b = -1$. We have $\lambda((1,1) \cdot (0,1) - 3/2) = \lambda ( - 1/2) = -1$, which holds if and only if $\lambda = 2$. We get $w = (2,2)$, and $b = -3$. One checks that $x_1, x_2$ lie on the hyperplanes satisfying
  $w \cdot x + b = 1$, and $x_3, x_4$ satisfying $w \cdot x + b = -1$. Now, in general we are free to rescale these as we'd like (we just chose to rescale in a convenient way).
  Thus, for $\lambda > 0$, we get $ (2\lambda, 2\lambda) \cdot (x,y) - 3 \lambda = 0$, which is the general equation of our hyperplane. Lastly, we have that the margin of this hyperplane is
  \begin{align*}
    \cfrac{w \cdot x_1 + b}{\norm{w}} = \cfrac{(2,2) \cdot (1,1) - 3}{\norm{(2,2)}} = \cfrac{1}{2\sqrt{2}} 
  \end{align*}
  If we remove $x_1$ from our sample, then our hyperplane is a horizontal line intersecting the y-axis at $3/2$. In this case,
  the support vectors become $x_2, x_3, x_5$, and the margin is $1/2$. If we remove $x_2$, the situation is unchanged, as $x_1, x_3, x_4$ are enough to define the hyperplane of points satisfying $w \cdot x + b$, for $w, b$ defined above. If we remove $x_3$, the marginal hyperplane is a horizontal line passing through $(0, 1/2)$, with support vectors $x_1, x_4, x_6 $, and margin $1/2$. If we remove $x_4$, the situation is unchanged, as $x_1, x_2, x_3$ constrain the hyperplane to be the one defined above. In general, it's possible to remove a point and increase the margin. We just saw it above, but we could also consider the following example. Let $(0,0)$ be labeled $-1$, $(1,0)$ be labeled $+1$, and $(M,0)$ be labeled $+1$, for $M>1$. Then, if we remove $(1,0)$, and let $M \to \infty$ the margin will tend to infinity too.
\end{proof}
\end{document}
